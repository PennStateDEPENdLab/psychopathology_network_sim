---
title: 'Network analyses of psychopathology in cross-section: conceptual and statistical
  challenges from a latent variable perspective'
author: "Michael Hallquist, Aidan Wright, & Peter Molenaar"
date: "2/20/2017"
output:
  html_document:
    theme: spacelab
    toc: yes
  html_notebook:
    theme: spacelab
    toc: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
options(width = 100)
rd <- file.path(getMainDir(), "psychopathology_network_sim")
if (!require(pacman)) { install.packages("pacman"); library(pacman) }
p_load(knitr, dplyr, ggplot2, tidyr, cowplot, simsem, bootnet, 
       qgraph, abind, foreach, doParallel, pander, viridis)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = rd)
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
panderOptions("digits", 3)
source(file.path(rd, "code/psychopathology_sim_functions.R"))
```

This document provides initial scaffolding for a conceptual critique of cross-sectional network approaches to psychopathology symptom data. In particular, we articulate the view that structural equation modeling (SEM) provides a statistical formalism that can accommodate a range of covariance structures, largely encompassing networks and also allowing one to examine alternative representations of latent structure. To highlight that nodal centrality metrics largely recapitulate free parameters in SEM, we conduct a series of simulations of latent variable models analyzed using a network approach, as well as simulated networks analyzed through latent variable models. We highlight conceptual and quantitative limitations of the network approach for resolving the latent structure of psychopathology and argue that principled model selection and comparison in SEM provides a strong basis for resolving alternative accounts of how symptoms covary with each other.

[[A point of some convergence is the recent Epskamp Psychometrika paper that blends latent variables and networks. This allows one to consider measurement error, to measure/test a hypothetical construct, and to compare model fit of a GGM against a SEM directly (i.e., same likelihood function). I suggest we include a summary of this later in the paper as a rapprochement and to highlight this as a promising direction.]]

# Conceptual critique

One of the most difficult challenges faced by clinicians and researchers alike is to conceptualize the co-occurrence of symptoms of psychopathology. Do certain symptoms co-occur because they reflect an underlying clinical entity or a shared latent trait? In a given individual, how should symptom co-occurrence inform treatment targets? Developing within-person models of symptom dynamics is an important research topic for clinical psychological science that may help to unpack the causal relationships among symptoms (Wright et al., 2016). For example, knowing that a given patient often becomes suicidal after a marital conflict may lead a clinician to make a referral for couples therapy or to work with the individual on interpersonal skills. At a between-persons level, anhedonia often precedes the development of suicidal ideation (Winer et al., 2014) and thus, on average, it may be beneficial to treat anhedonia before suicidal behavior develops.

Motivated by such questions, there has been a surge of interest in network analyses of psychopathology that could potentially reveal dynamic or causal relationships among symptoms (Borsboom et al., 2016; Hofmann, Curtiss, & McNally, 2016). More specifically, in cross-sectional samples, many research groups have used metrics derived from graph theory to explore whether particular symptoms are central in a network derived from psychometric indicators of one or more forms of psychopathology. Centrality within a symptom co-occurrence network, it has been argued, may help to identify important clinical targets that play a crucial role in precipitating other problems or that may be particularly salient indicators of a given disorder.

Although the motivation to understand dynamic relationships among features of psychopathology is well-founded, in this critical review, we highlight three important problems with applying network approaches to cross-sectional symptom data. First, the assumptions of network analyses (e.g., graphical lasso models) about the generative processes underlying the data are often not mentioned and may not align with many scientific questions. For example, networks based on partial correlations (i.e., where symptoms A and B are related conditioning on all other symptoms in the network) discard common covariation that may reflect a shared liability factor, a plausible and parsimonious explanation for symptom co-occurrence (Krueger & Markon, 2006). Second, because cross-sectional networks only represent between-persons variability, they are ill-suited to make conclusions about dynamic (i.e., temporally unfolding) relationships among symptoms at a within-person level. Finally, network analyses of cross-sectional symptom covariance matrices do not provide solid ground for making ontological inferences about the latent structure of psychopathology and therefore, have limited utility for advancing clinical theory and nomenclature.

Using a series of latent structure simulations, both network and latent trait, we demonstrate how network analyses provide descriptive accounts of latent structure that can be corroborated and tested more precisely using common latent variable analyses (e.g., latent trait models). We also demonstrate how network analyses are often vulnerable to making spurious inferences about the relationships among symptoms. Finally, we articulate how network analyses can be useful in guiding exploratory data analysis prior to testing formal models of latent structure.

## Basics of SEM versus GGM

In graph analyses of psychometric data, the Gaussian Graphical model (GGM) is the most common formal basis for network definition (Epskamp et al., in press, _Psychometrika_). Importantly, SEM is based on a model of the covariance matrix, whereas the GGM is based on the inverse covariance matrix (precision matrix), which measures partial correlations among two indicators net all other indicators. This has important implications for the assumed data generating process (e.g., that there is a unique relationship between two manifest indicators) and the representation of sparsity or constraint.

Under traditional SEM, sparsity is largely a function of the researcher's belief about the structure of the data. To the extent that a covariance matrix can be reasonably represented fewer than $\frac{p(p+1)}{2}$ parameters, then the model is overidentified (i.e., positive _df_). Thus, a common objective in SEM is to specify a model that is simpler than the saturated model but nevertheless represents the data well.

In the GGM, the estimation of edges (connections) among nodes is based on algorithms that attempt to optimize a criterion (often the extended BIC) by freeing different edges. To reduce spurious relationships and handle the problem of _p >> n_, it is common to regularize the network estimation using a LASSO penalty to control the sparsity of a network. Importantly, both the tuning operator $\lambda$ and the hyperparameter $\gamma$ that controls the additional complexity penalty on BIC (i.e., EBIC) are chosen by the researcher. Alternatively, several tuning parameters can be tested in order to identify a sparse network with the lowest EBIC. An important consideration here is that graphical LASSO (GLASSO) approaches are data driven algorithms to identify network structure with no input from the scientist on theoretically plausible parameters or constraints.

## Formal model versus secondary descriptive statistics

Network structures derived using a GLASSO + EBIC approach are then assumed to be sparser than the full-rank covariance matrix and are based on the precision matrix (as will be discussed below). Based on estimated edges in the network, researchers typically compute vertex (node) or edge (connection) metrics derived from graph theory. For example, the _strength_ of a node is defined as the sum of its connection weights to other nodes (inversely related to the assumed distance between two variables):

$$s_i = \sum_{j}W_{ij}$$

Such metrics describe the topology of the network, but are not integrally linked to the estimation of the edge structure itself. That is, the metrics are not measures of the likelihood of the parameters given the data and model. Rather, the statistical expectation of the covariance matrix is derived from the GLASSO + EBIC GGM estimation mentioned above and the free parameters are the estimated elements of the precision matrix $\Theta$. Likewise, in SEM, the parameters of the model jointly inform an estimate of the model-implied covariance matrix $\Sigma$. Given this structure, we wish to underscore that analyses of graph metrics such as strength or closeness are secondary statistics in that they build upon characteristics of the formal model parameters.

The closest analogy in SEM or traditional statistics would be analyses of the characteristics of free parameters such as factor loadings. For example, one might analyze whether a set of factor loadings are approximately normally distributed.

In secondary statistical analyses of networks and SEMs, it is not usually possible to estimate the uncertainty of a parameter as would be typical for primary parameters. For example, the strength centrality of a node is the sum of its weights, but there is no uncertainty about this sum. This leads to some difficulties in statistical analyses of graphs based on the covariance structure of a group insofar as it is difficult to compare differences between nodes or across groups in graph structure. As a result, applied graph analyses for psychometric data often find differences in graph metrics with limited ability to adjudicate the strength of evidence. To an extent, this can be circumvented using nonparametric bootstrapping procedures on the graph (Epskamp bootnet paper). By contrast, because the free parameters are typically of substantive interest in SEM and estimation uncertainty is an integral part of maximum likelihood estimation (based on observed Fisher information), it is straightforward to compare parameters or groups statistically.

## Ontological problems

In a broad sense, statistical models are intended to be a parsimonious representation of the observed data and, ideally, to generalize to the processes that caused the data. Many latent variable analyses seek to represent underlying causative factors that lead to the observed relationships among manifest variables. One formal definition of a latent variable is that it causes the associations among observed variables and that after conditioning on the latent variable, observed variables are independent (conditional independence definition; Lord 1953). Nonformal definitions of latent variables, however, often portray them as "hypothetical constructs" or simply data reduction devices that provide a parsimonious description of observed data (Harman 1960). These diverging views are the basis of critiques of latent variables (e.g., Borsboom 2001), as well as rejoinders that latent variables need not be literally causal (Jonas & Markon 2016). [Expand summary based on Bollen 2002 latent variable review.]

With respect to latent variable and network models, it is crucial to examine the ontology of latent variables and graphical models and to decide to what extent each provides a plausible description of psychometric symptom data (the focus here). In the case of the GGM, there are three important ontological assumptions:

1. Two symptoms (manifest variables) can be uniquely associated with each other without implying a latent clinical entity (e.g., depression) or dimensional trait (e.g., negative affect).
2. The relationship between two symptoms can best be understood when common variation with other symptoms is partialed out. That is, the symptoms should have some direct relationship with each other, rather than simply being associated through many indirect effect.
3. Manifest variables provide direct evidence of a symptom of interest, rather than being an indicator of a hypothetical construct. _Corollary_: measurement error is not important in characterizing the association among variables. Note that this assumption can be relaxed somewhat through latent variable + graph hybrids in which one models a GGM among latent variables (this is the generalized network psychometrics approach of Epskamp 2016 _Psychometrika_).

This quote from Hofmann Perspectives review provides a useful summary of the symptom network perspective: "Hence, a disorder is not the underlying cause of symptoms; it con- stitutes a network of symptoms that interact in ways that tend to maintain themselves. Accordingly, a stressful event does not activate an underlying entity called depres- sion, which then causes the emergence of symptoms. Rather, stressful events activate certain symptoms that, in turn, activate other symptoms, and when the requisite number of symptoms occurs, an episode of disorder may be diagnosable." (p. 2).

Although one can question the premises of taxonomic research on psychopathology, an effective critique would need to contend with the large literature on defining and measuring constructs in psychiatry. The classic Feighner criteria (Feighner 1972) provided a set of recommendations for describing mental disorders based on the premise that an underlying clinical entity (i.e., a latent variable) causes the observed symptoms. For example, hallucinations and delusions in a given patient are though to reflect an underlying thought disorder. This ontology has informed a host of taxonomic studies and revisions in the past 40 years, including the DSM-5 etc.  [Review McNally 2012 PTSD networks paper, which does get into ontology a bit more deeply... though it's badly flawed]. 

More generally, psychological research is built on a foundation of developing theories of hypothetical constructs (Cronbach & Meehl). Intelligence is not literally a score on a working memory test, but rather working memory performance is an indicator of some deeper aptitude. Intelligence research is a particularly good example of the latent variable debate in that there is strong evidence of a shared factor (whether causal or descriptive) that underlies performance on a host of ability tests, as well as some evidence of more specific abilities (the old Spearman _g_ versus _s_ discussion). Classical test theory has built on the premise that a hypothetical construct can be best described by measuring several indicators in order to estimate the true level of the construct. This approach also takes into account how observations can be corrupted by measurement noise while also providing an estimate of the trait level after accounting for such noise. Finally, test theory has given rise to a canonical approach to measuring and validating constructs in psychology (Clark & Watson 1995) that is fundamentally built on the premise that there are hypothetical constructs to be measured. SEM can be viewed in part as an extension of classical test theory for validating hypotheses about latent structure.

[Consider reviewing Krueger & Markon comorbidity annual review where they get into different latent->observed mappings across symptoms.]

To overthrow the hypothetical construct approach, a fundamentally different ontology would need to be proposed. Consider a multi-car 'pile-up' collision on an icy roadway. This phenomenon can be caused by a chain reaction of individuals trying, but failing, to brake in order to reduce speed and avoid a collision. Such a scenario need not appeal to a latent variable insofar as a set of observable causal conditions are sufficient to explain the collisions: 1) an icy road that minimizes friction, 2) a set of independent heavy objects traveling at speed, leading to kinetic energy, and 3) the inability of the work done on the car over a distance _d_ to equal its kinetic energy despite braking. Such causal dynamics have been appealed to in prior research on cross-sectional psychopathology symptom networks (Fried; Hofmann review), yet the dynamics that lead to observed scores on symptom measures are poorly understood.

We note that dynamic network models have an important place in disease research when the mechanisms of transmission are well understood. For example, diseases often spread through physical contact with an infected person, and predicting the rate and geographical spread of infection can help to contain dangerous outbreaks. Epidemiologists have used network models to understand how disease outbreak can be predicted by health-care utilization records in nearby geographical regions (Reis, Kohane, and Mandl, 2007 PLoS Medicine). Likewise, heterogeneity in the pattern and speed of disease outbreaks can be understood by modeling contact networks (Meyers et al., 2005, J Theoretical Biology). Our goal is not to undermine the utility of network models writ large, but to examine how symptom networks based on cross-sectional data can inform an understanding of the structure and taxonomy of psychopathology.

The representation of network analyses based on symptom covariance places manifest variables as the key atomic units of analysis and edges as the central explanatory parameters. [unpack]

[[Aidan: this would be a useful place to have your contributions on the cross-sectional dynamics problems]]

More specifically, a theory of psychopathology based on networks of symptoms would need to provide a plausible account of their associations. Ideally, such a theory would offer a hypothesis of how one symptom causes another, though this might depend on the pathophysiology, which could differ across symptoms. From an ontological perspective, it is not enough that a network approach provide a data-driven account of symptom covariance, even if the fit of such a model exceeds an alternative. This would only be a satisfactory solution if the primary goal of psychometric symptom modeling were to predict external outcomes such as social adjustment or probability of self-injury, for example. Traditionally, psychology has pursued explanatory theories that seek to describe the mechanisms that give rise to behavior (Yarkoni & Westfall, in press, Perspectives on Psyc Science). [Flesh out Yarkoni's points about why prediction is a valid goal, but also underscore that clinical utility requires a conceptual framework for diagnosis. Also: latent variable approaches are not incompatible with pure prediction, such as a horse race between a network and latent variable structure in prospectively predicting substance abuse relapse.]

## Model building and trimming in SEM 

Advocates of psychometric network analyses have criticized specific assumptions of the SEM approach that may not be satisfied in empirical data (Borsboom 2001). First, they note that SEM assumes that manifest variables are independent after conditioning on latent variables. If the associations among variables are more complex (e.g., residual associations after conditioning), this violation of conditional independence may render the interpretation of latent variables (e.g., factor scores) invalid. Second, they criticize the idea that symptoms are caused by a latent variable such as a disease state. Criticism of the latent variable premise is largely a reframing of the conditional independence assumption in that network proponents argue that symptoms are conditionally independent after conditioning on _each other_, not on a hypothetical construct.

The conditional independence assumption is evident in network analysis and latent variable models, including categorical latent variables (e.g., latent class models). In network models, as mentioned above, it is the association among symptoms that is assumed conditionally independent after conditioning on all other variables. In factor models, one assumes conditional independence of indicators after accounting for the factor loadings. And in latent class models, one assumes independence of indicators after conditioning on class membership. Crucially, however, there is a large literature on relaxing this assumption in latent variable models in order to accommodate more complex structures. For example, measurement invariance [PMI: unpack]. For example: allow residual association among items [unpack]. For example: exploratory SEM or Bayesian SEM with small positive priors on cross loadings [unpack].

Indeed, a standard component of model fitting in SEM is to examine modification indices based on the Lagrange multiplier in order to examine whether certain key parameters are omitted from the model. Simply freeing these without conceptual consideration is likely to overfit the data [citation?] and reduce the replicability of the findings. However, a principled consideration of modification indices may reveal plausible and justifiable reasons for adding parameters into the model that were not part of the researcher's initial hypothesis. [e.g., method variance or content similarity] The broader point is that modification indices are a well-worn tool for detecting violations of conditional independence in latent variable models. 

Likewise, an important recommendation in SEM is to fit several alternative models in order to adjudicate the relative evidence of one model over another (Burnham & Anderson, 2002). If the goal of statistical modeling is to provide a parsimonious and reasonable approximation of the observed data, then model selection serves as an important step to consider alternative hypotheses about the data generating processes. In empirical data, these processes are typically unknown and may be rather complex, but by comparing relative evidence of competing models, one can revise an explanatory theory in light of the extent to which the theoretically predicted model best characterizes the data. Likewise, comparing model-predicted versus observed data serves as a check on the quality of fit produced by the model (e.g., residual diagnostics or posterior predictive checks).

[[Peter: do you think that model search algorithms in SEM are worth reviewing as a data-driven counterpart to the EBIC + GLASSO network estimation approach? This seems like one avenue to demonstrate that a search algorithm can largely free relevant covariance parameters in order to approximate the original data, akin to the GGM. A difference would be the use of the precision matrix versus covariance matrix. ]]

A major challenge with network analyses is that the scientist has little control of the parameterization or form of the model. Rather, edges are estimated in the network according to regularization and relative fit criteria. Although simulation studies of this approach suggest that it recovers simulated networks reasonably well (Epskamp Arxiv paper 2016) in terms of sensitivity and specificity, one is not easily able to provide a priori input into the structure of the network or the plausibility of a given edge. For example, based on past theory and research, one may view fatigue and anhedonia as neurovegetative signs of depression and have a strong prior belief that these should be directly connected by an edge. However, to the extent that the association between these items reflects common inputs from other depressive features such as sleep disturbance or sadness, the GGM may select a model lacking the expected edge. Whether this is a 'feature' or a 'bug' of the method is in the eye of the beholder. In addition, although one can view the EBIC optimization procedure in terms of model selection (comparing networks with different LASSO or complexity penalties), it is not common to compare networks estimated by different methods or procedures to select the best representation. Neither is it common in the symptom network literature to compare the fit of a network model to a plausible latent variable model (cf. Epskamp 2016 Psychometrika, however). In summary, the scientist plays only a minimal role in model building in symptom network analyses, instead interpreting secondary statistics such as nodal centrality measures in order to understand potential explanations for covariance in the data.

## Network analyses of psychometric data: 

Some summary of the basic approach? Bootnet etc.?

## Networks cannot account for multiple causes, latent and observed

The conditional independence assumption in networks only permits unitary direct causes among items. As a result, if responses to an item reflect two or more processes, this is poorly represented. For example, there is a fairly developed literature on positively ("I tend to worry a lot") versus negatively keyed ("I don't fret life's troubles") indicating that the wording of each type tends to induce response similarity independent of the content of the construct. Thus, both of the items above could be strong indicators of negative affect, but failing to account for systematic effects of wording my misestimate the latent construct of interest.

Likewise, in a bifactor model, a general factor may account for common variance among many items (e.g., all items on a pain inventory) whereas orthogonal specific factors may capture systematic patterns of covariation in subsets of items (e.g., dull pain versus shooting pain). This structure is not readily represented by a network model (simulation below).

[Method variance, cross-loading, etc.]

## Latent variable models are network models (title temporarily, but blatantly, copied from your BBS commentary, Peter)

[The goal of this section is to a) introduce the formal mapping of latent variable models and network models, and b) suggest the idea that network metrics largely recapitulate parameters already present in SEM.]

[[Peter: here and elsewhere, it would be great to have your input on the link between these models, expanding on your BBS commentary. At present, the psychopathology network modelers are largely selling network models as a new tool that overcomes limitations of SEM. As you trenchantly argue in your commentary, however, modeling covariance in terms of observed variables alone is a rudimentary step that strongly assumes that the manifest variables are causative and that a more parsimonious representation cannot be obtained by considering latent causes.]]

Advocates of the network analysis approach (Hofmann 2015, 2016; McNally 2012) have argued that network analyses provide a perspective on symptom co-occurrence that is relatively free of assumptions of clinical entities or latent traits underlying the observed data. Moreover, this approach has been used to frame the centrality of a given symptom in theoretically meaningful terms. For example, if fatigue has high strength centrality in a network that includes mood and anxiety symptoms, it has been argued that this may reflect the importance of studying fatigue specifically [[Aidan: can you provide a better example from the papers you've encountered? This is just off the top of my head and I recall your having some juicy/silly examples]] as a causative factor in internalizing psychopathology. In this way, symptoms have become the focus of study such that symptoms that co-occur with many others may have greater conceptual importance than symptoms with more sparse patterns of co-occurrence.

We assert that network models applied to psychometric data may often recapitulate formal parameters of a related SEM while providing little unique information about the structure of the data. Nodal centrality metrics may serve as useful descriptive statistics to help a scientist develop a better sense of covariation patterns that would be hard to perceive in a large covariance matrix printed in tabular form. We view network analyses of observed items as a useful form of exploratory data analysis that builds on high-dimensional visualization tools (e.g., Emerson et al., 2012, Generalized Pairs Plot). However, such descriptive statistics and visualizations should not be conflated with formal models whose parameters seek to represent the underlying data. [this stuff may belong above in the formal model versus descriptive stats section]

Here, we consider how data generated from a variety of latent factor models (in SEM) is represented using network analyses. We also consider how data generated by a network model (the GGM) are represented from a latent variable perspective. We make three broad assertions about network analyses of data generated by latent variable models:

1. In latent variable models with simple structure (i.e., satisfying the conventional conditional independence assumption of SEM), centrality measures based on association strength (e.g., degree, strength, or eigenvector centrality) will be almost perfectly correlated with factor loadings.
2. In latent variable models that violate simple structure (either through cross-loadings or residual correlations among items), graph metrics will reflect a weighted sum of multiple parameters (e.g., a factor loading and a residual association), demonstrating the inability of this approach to consider multiple simultaneous causes of observed responses.
3. Network models based on the precision matrix (i.e., partial correlations) will tend to overemphasize variables with unique associations to other variables even when there is a strong positive manifold underlying the data. In particular, even when the majority of variance is explained by a general factor, relatively small residual relationships will tend to encourage spurious conclusions about the importance of certain variables in the network.

# Simulations of latent trait models through the network analysis lens
## Overview of simulations
All simulated data in this section are generated from confirmatory factor models fit within an SEM framework. The basic simulation structure uses standardidized factors (variance = 1.0) and equates item residual variances. Data are simulated using the `simsem` package in `R` based on `lavaan` syntax that generates the model-implied mean and covariance structure. The `simCFAGraphs` graphs function is the main worker that simulates data according to a factor model specification (in particular, $\lambda$, $\theta$, and $\Psi$ matrices) and fits the data using the `EBICglasso` function from the `qgraph` package. This follows the recommendations of a recent tutorial paper on generating psychopathology symptom networks (Epskamp: Estimating Psychological Networks and their Accuracy: A Tutorial Paper). Graphs are setup using the `igraph` package and centrality measures (strength, closeness, and betweenness at present to align with this literature) are derived.

## Hypothesis 1 simulations: strength is redundant with factor loadings

H1: In factor models with simple structure, strength centrality (or degree for binarized networks) will be redundant with factor loadings.

  * H1a: In one-factor models with simple structure, closeness centrality will be redundant with factor loadings. Closeness measures the average length of the shortest path between a target node and all other nodes in the graph.
  * H1b: The relationship between strength centrality and factor
    loadings will not diminish as the number of factors increases because
    strength is concerned with the summed connection magnitude and is not
    based on distance between nodes (building on path).

The premise of this hypothesis is that variables with higher factor loadings are better representations of the latent variable and will thus tend to be more strongly associated with other indicators of the latent variable. As a result, strength centrality (sum of association strengths) will be strongly associated with the factor loading.

In multi-factor solutions with (reasonably) simple structure, this will hold true as well because a variable that is connected to its primary factor will be associated with other items on the same factor in proportion to its loading.

### Basic one-factor simulation
Simulate data from a one-factor CFA with 10 indicators, 200 replications,
*N*=400, and factor loadings drawn from a random uniform sampler between .4 and
.95. Assume equal residual variance of 0.3 (30% unexplained variance). Twenty
"examples" are generated: an example in my shorthand is a replication of the
meta-structure to ensure that results are not peculiar to a given set of replications
or loadings.

```{r sim_1fac, cache=TRUE, include=FALSE}
if (file.exists("data/onefac_cfa_20examples_200reps_n400.RData")) {
  load("data/onefac_cfa_20examples_200reps_n400.RData")
} else {
dd <- demomaster(nexamples=20, nindicators=10, nfactors=1, nreplications=200, n=400, 
    loadingsampler=seq(.4, .95, .05), errorvars="eqresidvar", ivar=0.3, thetacorlist=NULL)
save(file="data/onefac_cfa_20examples_200reps_n400.RData", dd)
}
```

Means and standard deviations factor loadings correlations with graph centrality measures
```{r analyze_1fac, dependson="sim_1fac"}
##just get a mean and SD of correlations between each nodal measure and factor loadings
corrvgraph_glasso <- do.call(rbind, lapply(dd, function(example) { example$graph_v_factor$EBICglasso$corr_v_fitted }))
corrvgraph_pcor <- do.call(rbind, lapply(dd, function(example) { example$graph_v_factor$pcor$corr_v_fitted }))

kable(corrvgraph_glasso %>% group_by(measure, factor) %>% summarize_all(funs(mean, sd)), 
      digits=2, caption = "Association of GLASSO graph measures with 1-factor CFA loadings")
kable(corrvgraph_pcor %>% group_by(measure, factor) %>% summarize_all(funs(mean, sd)), 
      digits=2, caption = "Association of PCOR graph measures with 1-factor CFA loadings")

smat <- do.call(rbind, lapply(dd, function(rep) {
  brep <- filter(rep$graph_v_factor$EBICglasso$metric_v_loadings, node=="y1" & measure=="strength" &
                   fittedloading < 1)
}))

smat$graphNum <- rep(1:(nrow(smat)/2), each=2) #smat has two rows per replication. Need to identify these to get the spread to work properly

bb <- smat %>% select(graphNum, value, factor, fittedloading) %>% spread(key=factor, value=fittedloading) #graphNum
g <- ggplot(bb, aes(x=f1, y=value)) + geom_point(alpha=0.5) + stat_smooth(method="lm") + xlab("Fitted factor loading") +
  ylab("Strength centrality") + annotate(geom="text", x = 0.4, y=1.1, label="r = 0.97") +theme_cowplot(font_size=20)

pdf("strength 1-factor loadings.pdf", width=5, height=4)
plot(g)
dev.off()


kable(
  broom::tidy(cor.test(~ fittedloading + value, filter(smat, factor=="f1"))), 
  digits=2, caption="Association of strength with primary factor loading (f1)")


cmat <- do.call(rbind, lapply(dd, function(rep) {
  brep <- filter(rep$graph_v_factor$EBICglasso$metric_v_loadings, node=="y1" & measure=="closeness" &
                   fittedloading < 1)
}))

cmat$graphNum <- rep(1:(nrow(cmat)/2), each=2) #smat has two rows per replication. Need to identify these to get the spread to work properly

bb <- cmat %>% select(graphNum, value, factor, fittedloading) %>% spread(key=factor, value=fittedloading) #graphNum
g <- ggplot(bb, aes(x=f1, y=value)) + geom_point(alpha=0.3) + stat_smooth(method="lm") + xlab("Fitted factor loading") + ylab("Closeness centrality") + annotate(geom="text", x = 0.4, y=0.15, label="r = 0.96") +
  theme_cowplot(font_size=20)
pdf("closeness 1-factor loadings.pdf", width=5, height=4)
plot(g)
dev.off()

```

### Two-factor simulation
20 indicators (10 per factor) with random loadings and equal residual variances as above. Other settings as above.
```{r sim_2fac, cache=TRUE, include=FALSE}
if (file.exists("data/twofac_cfa_20examples_200reps_n400.RData")) {
  load("data/twofac_cfa_20examples_200reps_n400.RData")
} else {
dd2 <- demomaster(nexamples=20, nindicators=20, nfactors=2, nreplications=200, n=400, 
    loadingsampler=seq(.4, .95, .05), errorvars="eqresidvar", ivar=0.3, thetacorlist=NULL)
save(file="data/twofac_cfa_20examples_200reps_n400.RData", dd2)
}
```

Note that GLASSO maintains a high correlation in the two-factor case, whereas a simple partial correlation basis
begins to diminish in its linear dependency with loadings.
```{r analyze_2fac, dependson="sim_2fac"}
corrvgraph_glasso <- do.call(rbind, lapply(dd2, function(example) { example$graph_v_factor$EBICglasso$corr_v_fitted }))
corrvgraph_pcor <- do.call(rbind, lapply(dd2, function(example) { example$graph_v_factor$pcor$corr_v_fitted }))

kable(corrvgraph_glasso %>% group_by(measure, factor) %>% summarize_all(funs(mean, sd)), 
      digits=2, caption = "Association of GLASSO graph measures with 2-factor CFA loadings")
kable(corrvgraph_pcor %>% group_by(measure, factor) %>% summarize_all(funs(mean, sd)),
      digits=2, caption = "Association of PCOR graph measures with 2-factor CFA loadings")
```

```{r dependson="sim_2fac", echo=FALSE}
smat <- do.call(rbind, lapply(dd2, function(rep) {
  brep <- filter(rep$graph_v_factor$EBICglasso$metric_v_loadings, node=="y2" & measure=="strength")
}))

smat$graphNum <- rep(1:(nrow(smat)/2), each=2) #smat has two rows per replication. Need to identify these to get the spread to work properly

bb <- smat %>% select(graphNum, value, factor, fittedloading) %>% spread(key=factor, value=fittedloading) #graphNum
ggplot(bb, aes(x=f1, y=value)) + geom_point() + stat_smooth()
kable(
  broom::tidy(cor.test(~ fittedloading + value, filter(smat, factor=="f1"))), 
  digits=2, caption="Association of strength with primary factor loading (f1)")

```

## Hypothesis 2: Centrality metrics will capture the joint magnitude of cross-loadings in 2+ factor CFA violating simple structure

H2: In a 2-factor (or larger) structure, a cross-loading will tend to increase the centrality of that variable in proportion to the joint magnitude of
its factor loadings. This highlights a limitation of network analyses in that multiple causes are not accommodated.

For the first simulation, we specify a 2-factor model (9 per factor) with fixed 0.6 loadings. Items y1-y9 loading onto f1, and y10-y18 load onto f2. We then randomly vary the magnitude of a cross-loading of y2 onto f2. 500 replications of this structure with *N*=400 are simulated.

```{r sim_2fac_cross, cache=TRUE, include=FALSE}
##need to lock in a given factor structure, then vary the cross-loading magnitude.

#consistent 2-factor structure
lambda <- as.matrix(rbind(
        c(rep(0.6, 9), rep(0, 9)), #f1
        c(rep(0, 9), rep(0.6, 9)))) #f2

varnames <- paste0("y", 1:ncol(lambda))

fvar <- c(1.0, 1.0) #factor variance (standardized)
errorvars <- rep(0.3, ncol(lambda)) #fixed error specification
#errorvars <- computeResidvar(targetitemvar=1.0, lambda, fvar=fvar) #compute item residual variances assuming equal observed variances 
theta <- diag(as.vector(errorvars)) #0 resid cov initially
rownames(theta) <- colnames(theta) <- varnames #necessary for addErrorCor to work

psi <- diag(fvar) #zero covariance in factor structure at the moment
dimnames(psi) <- list(f=paste0("f", 1:nrow(psi)), f=paste0("f", 1:ncol(psi)))

#model specification structure. Currently just wrapping up individual arguments above into a list
model1 <- list(
    varnames=varnames, #vector of variable names
    lambda=lambda, #nitems x nfactors loadings matrix
    theta=theta, #covariance (residual) matrix for observed items
    psi=psi #covariance matrix for factors
)

#add one cross-loading in 500 replications
cl <- makeCluster(8) #defaults to PSOCK cluster
clusterExport(cl, c("simCFAGraphs"))

registerDoParallel(cl)
clusterEvalQ(cl, library(dplyr)) #load dplyr in all workers
mlist <- foreach(i=1:500) %dopar% {
      mm <- model1
      mm$lambda[2, 2] <- runif(1, 0.2, 0.8) #add cross-loading of y2 onto f2
      sims <- simCFAGraphs(mm, nreplications=5, n=400, thetastart=TRUE, parallel=0)
      return(sims)
}
stopCluster(cl)
```

```{r, dependson="sim_2fac_cross", include=FALSE}
#wrangling of output structure
#will return a data.frame with betweenness for y2 (cross-loaded item) and the population
#and fitted loadings for both factors
bmat <- do.call(rbind, lapply(mlist, function(rep) {
  brep <- filter(rep$graph_v_factor$EBICglasso$metric_v_loadings, node=="y2" & measure=="betweenness")
}))

bmat$graphNum <- rep(1:(nrow(bmat)/2), each=2) #bmat has two rows per replication. Need to identify these to get the spread to work properly

smat <- do.call(rbind, lapply(mlist, function(rep) {
  brep <- filter(rep$graph_v_factor$EBICglasso$metric_v_loadings, node=="y2" & measure=="strength")
}))

smat$graphNum <- rep(1:(nrow(smat)/2), each=2) #smat has two rows per replication. Need to identify these to get the spread to work properly

cmat <- do.call(rbind, lapply(mlist, function(rep) {
  brep <- filter(rep$graph_v_factor$EBICglasso$metric_v_loadings, node=="y2" & measure=="closeness")
}))

cmat$graphNum <- rep(1:(nrow(cmat)/2), each=2) #smat has two rows per replication. Need to identify these to get the spread to work properly
```

### Betweenness centrality

We do not see support for an association of the cross-loading magnitude with node betweenness.

```{r dependson="sim_2fac_cross", echo=FALSE}
#correlation of node betweenness and core factor
kable(
  broom::tidy(cor.test(~ fittedloading + value, filter(bmat, factor=="f1"))), 
  digits=2, caption="Association of betweenness with primary factor loading (f1)")

bb <- bmat %>% select(graphNum, value, factor, fittedloading) %>% spread(key=factor, value=fittedloading) #graphNum contains the identifying column
#ggplot(bb, aes(x=f1, y=value)) + geom_point() + stat_smooth()
```

Correlation of fitted loading with secondary factor (runif 0.2--0.8)

```{r dependson="sim_2fac_cross", echo=FALSE}
#correlation of node betweenness and core factor
kable(broom::tidy(cor.test(~ fittedloading + value, filter(bmat, factor=="f2"))),
      digits=2, caption="Association of betweenness with cross-loading (f2)")
#ggplot(bb, aes(x=f2, y=value)) + geom_point() + stat_smooth()
```

Joint prediction of betweenness by both loadings
```{r dependson="sim_2fac_cross", echo=FALSE}
pander(summary(lm(value ~ f1 + f2, bb)))
```

### Closeness centrality

The magnitude of the cross-loading does strongly influence the closeness centrality of the target node y2.

Correlation of closeness with fitted primary factor loading (loading varies ~0.6)

```{r dependson="sim_2fac_cross", echo=FALSE, message=FALSE}
#correlation of node betweenness and core factor
kable(broom::tidy(cor.test(~ fittedloading + value, filter(cmat, factor=="f1"))),
      digits=2, caption="Association of closeness with primary loading (f1)")

bb <- cmat %>% select(graphNum, value, factor, fittedloading) %>% spread(key=factor, value=fittedloading) #graphNum contains the identifying column
#ggplot(bb, aes(x=f1, y=value)) + geom_point() + stat_smooth()
```

Correlation of closeness with fitted secondary factor loading (runif 0.2--0.8)

```{r dependson="sim_2fac_cross", echo=FALSE}
#correlation of node betweenness and core factor
kable(broom::tidy(cor.test(~ fittedloading + value, filter(cmat, factor=="f2"))),
      digits=2, caption="Association of closeness with secondary loading (f2)")
      
ggplot(bb, aes(x=f2, y=value)) + geom_point() + stat_smooth()
```

Joint prediction of closeness by both loadings
```{r dependson="sim_2fac_cross"}
pander(summary(lm(value ~ f1 + f2, bb)))
```

### Strength centrality
Correlation of strength with fitted primary factor loading (loading varies ~0.6)

```{r dependson="sim_2fac_cross"}
#correlation of node betweenness and core factor
broom::tidy(cor.test(~ fittedloading + value, filter(smat, factor=="f1")))

bb <- smat %>% select(graphNum, value, factor, fittedloading) %>% spread(key=factor, value=fittedloading) #graphNum contains the identifying column
#ggplot(bb, aes(x=f1, y=value)) + geom_point() + stat_smooth()
```

Correlation of fitted loading with secondary factor (runif 0.2--0.8)

```{r dependson="sim_2fac_cross", echo=FALSE}
#correlation of node betweenness and core factor
kable(broom::tidy(cor.test(~ fittedloading + value, filter(smat, factor=="f2"))),
      digits=2, caption="Association of strength with secondary loading (f2)")
ggplot(bb, aes(x=f2, y=value)) + geom_point() + stat_smooth()
```

Joint prediction of betweenness by both loadings
```{r dependson="sim_2fac_cross", echo=FALSE}
pander(summary(lm(value ~ f1 + f2, bb)))
```


### H2a: What if we allow the loading on each factor to vary?

Here, instead of varying the magnitude of one cross-loading while the other remains fixed, we simulate data along a 2-D grid in which
the factor loading of y2 on f1 varies from 0.2--0.8 and its factor loading on f2 also varies from 0.2--0.8. The hypothesis is that centrality measures will be a joint function of the loading magnitudes such that high loadings on both will be associated with high centrality. This is a more powerful demonstration of the point above that network metrics do not allow one to unmix multiple causes.

```{r sim_2fac_cross_rand, cache=TRUE, include=FALSE}
##need to lock in a given factor structure, then vary the cross-loading magnitude.

#consistent 2-factor structure
lambda <- as.matrix(rbind(
        c(rep(0.6, 9), rep(0, 9)), #f1
        c(rep(0, 9), rep(0.6, 9)))) #f2

varnames <- paste0("y", 1:ncol(lambda))

fvar <- c(1.0, 1.0) #factor variance (standardized)
errorvars <- rep(0.3, ncol(lambda)) #fixed error specification
#errorvars <- computeResidvar(targetitemvar=1.0, lambda, fvar=fvar) #compute item residual variances assuming equal observed variances 
theta <- diag(as.vector(errorvars)) #0 resid cov initially
rownames(theta) <- colnames(theta) <- varnames #necessary for addErrorCor to work

psi <- diag(fvar) #zero covariance in factor structure at the moment
dimnames(psi) <- list(f=paste0("f", 1:nrow(psi)), f=paste0("f", 1:ncol(psi)))

#model specification structure. Currently just wrapping up individual arguments above into a list
model1 <- list(
    varnames=varnames, #vector of variable names
    lambda=lambda, #nitems x nfactors loadings matrix
    theta=theta, #covariance (residual) matrix for observed items
    psi=psi #covariance matrix for factors
)

cl <- makeCluster(8) #defaults to PSOCK cluster
clusterExport(cl, c("simCFAGraphs"))

registerDoParallel(cl)
clusterEvalQ(cl, library(dplyr)) #load dplyr in all workers
loading_grid <- expand.grid(f1=seq(0.2, 0.8, .05), f2=seq(0.2, 0.8, 0.05))
mlist_rand <- foreach(i=iter(loading_grid, by='row')) %dopar% {
      mm <- model1
      mm$lambda[1, 2] <- i$f1 #runif(1, 0.2, 0.8) #random loading of y2 onto f1
      mm$lambda[2, 2] <- i$f2  #runif(1, 0.2, 0.8) #random loading of y2 onto f2
      sims <- simCFAGraphs(mm, nreplications=20, n=400, thetastart=TRUE, parallel=0)
      return(sims)
}
stopCluster(cl)
```

```{r cross_rand_stats, dependson="sim_2fac_cross_rand", include=FALSE, cache=TRUE}
#will return a data.frame with betweenness for y2 (cross-loaded item) and the population
#and fitted loadings for both factors

bmat_rand <- do.call(rbind, lapply(mlist_rand, function(rep) {
  brep <- filter(rep$graph_v_factor$EBICglasso$metric_v_loadings, node=="y2" & measure=="betweenness")
}))

bmat_rand$graphNum <- rep(1:(nrow(bmat_rand)/2), each=2) #bmat has two rows per replication. Need to identify these to get the spread to work properly

smat_rand <- do.call(rbind, lapply(mlist_rand, function(rep) {
  brep <- filter(rep$graph_v_factor$EBICglasso$metric_v_loadings, node=="y2" & measure=="strength")
}))

smat_rand$graphNum <- rep(1:(nrow(smat_rand)/2), each=2) #smat has two rows per replication. Need to identify these to get the spread to work properly

cmat_rand <- do.call(rbind, lapply(mlist_rand, function(rep) {
  brep <- filter(rep$graph_v_factor$EBICglasso$metric_v_loadings, node=="y2" & measure=="closeness")
}))

cmat_rand$graphNum <- rep(1:(nrow(cmat_rand)/2), each=2) #smat has two rows per replication. Need to identify these to get the spread to work properly

#look at how the correlation matrix is affected
corr_rand <- do.call(rbind, lapply(mlist_rand, function(rep) {
  wif1 <- rep$adjmats$pearson$average[c(1,3:9), c(1,3:9)]
  wif1m <- mean(wif1[lower.tri(wif1)])
  wif2 <- rep$adjmats$pearson$average[10:18, 10:18]
  wif2m <- mean(wif2[lower.tri(wif2)])
  bwf1f2m <- mean(rep$adjmats$pearson$average[c(1,3:9), 10:18])
  f1y2m <- mean(rep$adjmats$pearson$average[2, c(1,3:9)]) #correlation of target item (y2) with other items on f1
  f2y2m <- mean(rep$adjmats$pearson$average[2, 10:18]) #correlation of target item (y2) with other items on f2
  
  data.frame(wif1m=wif1m, wif2m=wif2m, bwf1f2m=bwf1f2m, f1y2m=f1y2m, f2y2m=f2y2m,
             f1loading=select(rep[["simsemout"]]@coef, matches("f1=~y2")),
             f2loading=select(rep[["simsemout"]]@coef, matches("f2=~y2")))
}))

```

```{r, dependson="cross_rand_stats"}
Hmisc::rcorr(as.matrix(corr_rand))

#summary(lm(bwf1f2m ~ f1..y2 * f2..y2, corr_rand))
```

Conclusions about effects of cross-loading on correlations among items:

1. Higher factor loading of y2 on f1 goes with higher average correlation (r = .90) of y2 with y1-y9 (other f1 items): obvious
2. Converse: Higher factor loading of y2 on f2 goes with higher average correlation (r = .91) of y2 with y10-y18 (other f2 items): obvious
3. Higher factor loading of y2 on f2 goes with lower average correlation (r = -.35) on f1. So loading drives similarity to primary factor and dissimilarity to cross-loaded factor. Converse holds, too (y2 on f1)
4. Non-trivial: Higher correlation of y2 with f1 items associated with lower correlation with f2 items, r = -.63.

```{r dependson="cross_rand_stats"}
kable(broom::tidy(cor.test(~ fittedloading + value, filter(bmat_rand, factor=="f2"))), digits=2, caption="Correlation of betweenness with f2 fitted loading")
kable(broom::tidy(cor.test(~ fittedloading + value, filter(bmat_rand, factor=="f2"))), digits=2, caption="Correlation of betweenness with f1 fitted loading")

kable(broom::tidy(cor.test(~ fittedloading + value, filter(cmat_rand, factor=="f2"))), digits=2, caption="Correlation of closeness with f2 fitted loading")
kable(broom::tidy(cor.test(~ fittedloading + value, filter(cmat_rand, factor=="f1"))), digits=2, caption="Correlation of closeness with f1 fitted loading")

kable(broom::tidy(cor.test(~ fittedloading + value, filter(smat_rand, factor=="f2"))), digits=2, caption="Correlation of strength with f2 fitted loading")
kable(broom::tidy(cor.test(~ fittedloading + value, filter(smat_rand, factor=="f1"))), digits=2, caption="Correlation of strenght with f1 fitted loading")

#bb_rand <- smat_rand %>% select(graphNum, value, factor, fittedloading) %>% spread(key=factor, value=fittedloading)
bb_rand <- smat_rand %>% select(graphNum, value, factor, poploading) %>% spread(key=factor, value=poploading) #graphNum contains the identifying column
#ggplot(bb_rand, aes(x=f1, y=value)) + geom_point() + stat_smooth()
pander(summary(lm(value ~ f1 + f2, bb_rand))) #so measure becomes hugely predicted by each loading

#aggregate replications within each combination of f1 and f2loading
bbagg <- bb_rand %>% group_by(f1, f2) %>% summarize(value=mean(value))

#3d plot of sorts
pdf("strength f1f2.pdf", width=7, height=5)
ggplot(bbagg, aes(x=f1, y=f2, fill=value)) + geom_tile() +
  #scale_color_gradient() + 
  scale_fill_viridis("Strength", option="viridis") +
  xlab("Factor 1 loading") + ylab("Factor 2 loading") +
  theme_cowplot(font_size=20)
  #ggtitle("Association of f1 (x) and f2 (y) loadings with strength (color)")
dev.off()
#geom_jitter(size=3, position = position_jitter(width=0.03, height=0.03)) + 
```


## Hypothesis 3: residual correlations between items will strongly influence centrality measures

Simulate residual correlations on 2-D grid of cross-loadings for four items: y1, y2 (f1); y10, y11 (f2)
Residual correlations range in strength from 0.0 - 0.8 in .05 increments while holding constant the primary factor loadings at 0.6.
```{r sim_2fac_residcorr, cache=TRUE, include=FALSE}
cl <- makeCluster(8) #defaults to PSOCK cluster
clusterExport(cl, c("simCFAGraphs"))

registerDoParallel(cl)
clusterEvalQ(cl, library(dplyr)) #load dplyr in all workers
loading_grid <- expand.grid(cl1=seq(0.0, 0.8, .05), cl2=seq(0.0, 0.8, 0.05))
mlist_crossload <- foreach(i=iter(loading_grid, by='row')) %dopar% {
      mm <- model1
      mm$theta <- addErrorCor(mm$theta, c("y1", "y10"), i$cl1)
      mm$theta <- addErrorCor(mm$theta, c("y2", "y11"), i$cl2)
      sims <- simCFAGraphs(mm, nreplications=5, n=400, thetastart=TRUE, parallel=0)
      return(sims)
}
stopCluster(cl)
```

```{r dependson="sim_2fac_residcorr"}
#@smat_rand <- do.call(rbind, 
smat_rand <- do.call(bind_rows, lapply(mlist_crossload, function(rep) {
  #filter to only those items that allowed resid corr
  srep <- filter(rep$graph_v_factor$EBICglasso$metric_v_loadings, node %in% c("y1", "y2", "y10", "y11") & measure=="closeness")
})) #bind_rows from dplyr automatically fills NAs (this is needed here since 0, 0 is one cell in the crossloading)

pander(summary(lm(value ~ fittedloading, filter(smat_rand, node=="y1"))))
#pander(summary(lm(value ~ rcorr_pop, filter(smat_rand, node=="y1"))))
pander(summary(lm(value ~ rcorr_fitted, filter(smat_rand, node=="y1"))))
pander(summary(lm(value ~ rcorr_fitted + I(rcorr_fitted^2), filter(smat_rand, node=="y1"))))
pander(summary(lm(value ~ rcorr_fitted * fittedloading, filter(smat_rand, node=="y1"))))
#correlations are added to theta by r * sqrt(V1)*sqrt(V2) to respect the residual variances of the indicators
#to scale back onto correlation, we undo this by standardization: b / [sqrt(V1) * sqrt(V2)]
#here we have indicators of 0.3 in both cases, so just divide by 0.3. Technically should use fitted vars
#to get this exactly right, but this is a trivial difference
#range(smat_rand$rcorr_pop/0.3, na.rm=T)
ggplot(smat_rand, aes(x=rcorr_fitted/0.3, y=value, color=fittedloading)) + facet_wrap(~node) + geom_point() + 
  ggtitle("Association of estimated residual correlation (x) with strength centrality (y)") + xlab("Estimated residual correlation of variable")

smat_rand$graphNum <- rep(1:(nrow(smat_rand)/2), each=2) #smat has two rows per replication. Need to identify these to get the spread to work properly


#bb_rand <- smat_rand %>% select(graphNum, value, factor, poploading) %>% spread(key=factor, value=poploading) #graphNum contains the identifying column
#ggplot(bb_rand, aes(x=f1, y=value)) + geom_point() + stat_smooth()
#pander(summary(lm(value ~ f1 + f2, bb_rand))) #so measure becomes hugely predicted by each loading


###

#should probably reshape to allow for MLM where nodes are random
#initial corroboration for hypothesis 2
# mm <- do.call(rbind, mlist)
# my1 <- filter(mm, node=="y1")
# summary(lm(value ~ y10, my1)) #betweenness of y1 predicted by residual correlation with y10
# cor.test(~ value + y10, my1) #r ~ 0.55
# 
# my10 <- filter(mm, node=="y10")
# summary(lm(value ~ y1, my10)) #betweenness of y10 predicted by residual correlation with y1
# cor.test(~ value + y1, my10)
```

## Next steps (in development)

An important consideration that is unexplored in these initial simulations is the importance of characterizing subgraph structure, often by parcelling into connected subcomponents. Or in graph analyses, this is the problem of community detection (of which there are several algorithms). Granted, these algorithms are largely inline with factor analyses! Regardless, in graphs with moderate to high modularity, metrics within and between module should not be confounded by aggregated statistics.

*Bifactor example*

*Disproportionate influence of residual associations in the presence of positive manifold.* This is largely a problem of the partial correlation/precision idea.

*Considerations of subgraph structure.* Metrics are often reported blind to the idea of subgraphs, network experts rarely overlook considerations of modularity, connected components, and so on. This pulls us back toward factor models vs. community detection, which is another parallel

## Hypothesis: GLASSO or PCOR will overemphasize unique associations to the exclusion of commonality

So, we need to basically parametrically manipulate the amount of variation explained by a latent variable versus a unique bivariate association.

The cleanest example might be a 2-factor structure with a single residual association across factors. Does the relative magnitude of this association scale with the strength of the GLASSO edge?

Or maybe even a 1-factor structure with a residual association. Does it look dramatically different from a one-factor model?



```{r sim_2fac_eqvar, cache=TRUE, include=FALSE}
##need to lock in a given factor structure, then vary the cross-loading magnitude.

#consistent 2-factor structure

#goal is to maintain equal item variance, but explain different proportions of that variance according to
# factor loadings... So basically hold constant prop var at certain tiers, but vary which factor explains it.

#what if we use an R2 of 60%?
r2 <- .64

lambda <- as.matrix(rbind(
        c(rep(sqrt(r2), 9), rep(0, 9)), #f1
        c(rep(0, 9), rep(sqrt(r2), 9)))) #f2

varnames <- paste0("y", 1:ncol(lambda))

fvar <- c(1.0, 1.0) #factor variance (standardized)
#errorvars <- rep(0.3, ncol(lambda)) #fixed error specification
errorvars <- computeResidvar(targetitemvar=1.0, lambda, fvar=fvar) #compute item residual variances assuming equal observed variances 
theta <- diag(as.vector(errorvars)) #0 resid cov initially
rownames(theta) <- colnames(theta) <- varnames #necessary for addErrorCor to work

psi <- diag(fvar) #zero covariance in factor structure at the moment
dimnames(psi) <- list(f=paste0("f", 1:nrow(psi)), f=paste0("f", 1:ncol(psi)))

#model specification structure. Currently just wrapping up individual arguments above into a list
model1 <- list(
    varnames=varnames, #vector of variable names
    lambda=lambda, #nitems x nfactors loadings matrix
    theta=theta, #covariance (residual) matrix for observed items
    psi=psi #covariance matrix for factors
)

cl <- makeCluster(8) #defaults to PSOCK cluster
clusterExport(cl, c("simCFAGraphs"))

registerDoParallel(cl)
clusterEvalQ(cl, library(dplyr)) #load dplyr in all workers

#f1loading <- seq(0.15, 0.80, .01)
#f2loading <- sqrt(r2 - f1loading^2 + .Machine$double.eps) #to avoid slightly negative numbers
#f1loading^2 + f2loading^2

#rework the loading grid to be in step size of variance explained .01 increment
f1loading <- sqrt(seq(0, .64, .01))
f2loading <- sqrt(r2 - f1loading^2 + .Machine$double.eps)
#f1loading^2 + f2loading^2
loading_grid <- data.frame(f1loading, f2loading)

#plot(loading_grid$f1loading)

mlist_rand <- foreach(i=iter(loading_grid, by='row')) %dopar% {
      mm <- model1
      mm$lambda[1, 2] <- i$f1 #runif(1, 0.2, 0.8) #random loading of y2 onto f1
      mm$lambda[2, 2] <- i$f2  #runif(1, 0.2, 0.8) #random loading of y2 onto f2
      sims <- simCFAGraphs(mm, nreplications=50, n=400, thetastart=TRUE, parallel=0)
      return(sims)
}
stopCluster(cl)


#mlist_rand[[10]]$graph_v_factor$EBICglasso$metric_v_loadings

ebicall <- do.call(rbind, lapply(1:length(mlist_rand), function(g) { 
  df <- subset(mlist_rand[[g]]$graph_v_factor$EBICglasso$corr_v_fitted, measure=="strength") #%>%
    #spread(key=factor, value=mcv)
  #browser()
  df$f1loading <- loading_grid[g, 1]
  df$f2loading <- loading_grid[g, 2]
  df
}))

#ggplot(ebicall, aes(x=f1loading, y=f2loading, color=mcv)) + geom_point() + facet_wrap(~factor)
library(viridis)
pdf("/Users/michael/Documents/fload.pdf", width=12, height=12)
ggplot(ebicall, aes(x=f1loading^2, y=f2loading^2, color=mcv)) + geom_point() + facet_wrap(~factor) +
  scale_color_viridis("magma")
dev.off()
#so now, we look at the extent to which centrality and primary factor loading are associated.
#we should see centrality redundancy with factor loadings falling apart as cross-loading increases.
#is this 

```

On further reflection, the approach above of varying the cross-loading magnitude in terms of percent variance is a relatively useful, but redundant, extension of the cross-loading example above (that multiple causes cannot be accommodated). What we really need is a true shared variance versus unique cause situation.

I think this is probably cleanest through a two-factor CFA with an additional factor that scales the unique association of two items.

TODO: What happens if factors start to correlate? Do we start to overvalue cross-factor associations over within-factor associations? This would be the broadband prediction of the partial correlation failure notion.

```{r fixedvar_cross, cache=TRUE, include=FALSE}
#what if we use an R2 of 64%?
r2 <- .64
perfac <- 10
# lambda <- as.matrix(rbind(
#         c(rep(sqrt(r2), perfac), rep(0, perfac*2)), #f1
#         c(rep(0, perfac), rep(sqrt(r2), perfac), rep(0, perfac)),  #f2
#         c(rep(0, perfac*2), rep(sqrt(r2), perfac)),  #f3
#         c(rep(0, perfac*3)))) #u

# lambda <- as.matrix(rbind(
#         c(rep(sqrt(r2), perfac), rep(0, perfac + 1)), #f1
#         c(rep(0, perfac), rep(sqrt(r2), perfac), 0),  #f2
#         c(rep(0, perfac*2), 0.8))) #u with one fixed loading of 0.8

lambda <- as.matrix(rbind(
        c(rep(sqrt(r2), perfac), rep(0, perfac)), #f1
        c(rep(0, perfac), rep(sqrt(r2), perfac)),  #f2
        c(rep(0, perfac*2)))) #u

lambdaconstraint <- matrix(0, nrow=nrow(lambda), ncol=ncol(lambda))
lambdaconstraint[3, c(2,11)] <- 1 #constrain these loadings to equality in fitting (loading matrix contains integer identifiers for equated params)
varnames <- paste0("y", 1:ncol(lambda))

fvar <- c(1.0, 1.0, 1.0) #factor variance (standardized)
#errorvars <- rep(0.3, ncol(lambda)) #fixed error specification
errorvars <- computeResidvar(targetitemvar=1.0, lambda, fvar=fvar) #compute item residual variances assuming equal observed variances 
theta <- diag(as.vector(errorvars)) #0 resid cov initially
rownames(theta) <- colnames(theta) <- varnames #necessary for addErrorCor to work

psi <- diag(fvar) #zero covariance in factor structure at the moment
dimnames(psi) <- list(f=paste0("f", 1:nrow(psi)), f=paste0("f", 1:ncol(psi)))

#model specification structure. Currently just wrapping up individual arguments above into a list
model1 <- list(
    varnames=varnames, #vector of variable names
    lambda=lambda, #nitems x nfactors loadings matrix
    lambdaconstraint=lambdaconstraint, #constraint matrix for factor loadings
    theta=theta, #covariance (residual) matrix for observed items
    psi=psi #covariance matrix for factors
)

cl <- makeCluster(4) #defaults to PSOCK cluster
clusterExport(cl, c("simCFAGraphs"))

registerDoParallel(cl)
clusterEvalQ(cl, library(dplyr)) #load dplyr in all workers

#f1loading <- seq(0.15, 0.80, .01)
#f2loading <- sqrt(r2 - f1loading^2 + .Machine$double.eps) #to avoid slightly negative numbers
#f1loading^2 + f2loading^2

#rework the loading grid to be in step size of variance explained .01 increment
f_loading <- sqrt(seq(0, r2, .01))
u_loading <- sqrt(r2 - f_loading^2 + .Machine$double.eps)
f_loading^2 + u_loading^2
loading_grid <- data.frame(f_loading, u_loading)

#now build a 2-D grid for the unique factor based on the combinations
#this sets up simulations in which y2 and y11 have variation in how much
#their variation is explained by common factor variance (f1 or f2) or unique
#variance (u).
# fu_grid <- expand.grid(f1=paste(f_loading, u_loading, sep="_"),
#                        f2=paste(f_loading, u_loading, sep="_")) %>%
#   separate(f1, into=c("f1_loading", "u1_loading"), sep="_") %>%
#   separate(f2, into=c("f2_loading", "u2_loading"), sep="_") %>%
#   mutate_at(vars(f1_loading, f2_loading, u1_loading, u2_loading), funs(as.numeric))

# fu_grid <- fu_grid[1:100,]
#mlist_uniq <- foreach(i=iter(fu_grid, by='row')) %dopar% {

#update: for identification, equate the loading of y2 and y11 on u (2-variable factor)
mlist_uniq <- foreach(i=iter(loading_grid, by='row')) %dopar% {
      mm <- model1
      # mm$lambda[1, 2] <- i$f1_loading #loading of y2 on f1
      # mm$lambda[3, 2] <- i$u1_loading #loading of y2 on u
      # mm$lambda[2, 11] <- i$f2_loading #loading of y11 on f2
      # mm$lambda[3, 11] <- i$u2_loading #loading of y11 on u
      
      mm$lambda[1, 2] <- i$f_loading #loading of y2 on f1
      mm$lambda[3, 2] <- i$u_loading #loading of y2 on u
      mm$lambda[2, 11] <- i$f_loading #loading of y11 on f2
      mm$lambda[3, 11] <- i$u_loading #loading of y11 on u
      sims <- simCFAGraphs(mm, nreplications=20, n=400, thetastart=TRUE, parallel=0, saveLavObj = FALSE)
      
      return(sims)
}
stopCluster(cl)
```

```{r dependson="fixedvar_cross"}
y2eff <- do.call(rbind, lapply(mlist_uniq, function(cellreps) {
  filter(cellreps$graph_v_factor$EBICglasso$metric_v_loadings, node=="y2")
}))

cor.test(~value + poploading, subset(y2eff, factor=="f3" & measure=="betweenness"))
cor.test(~value + poploading, subset(y2eff, factor=="f1" & measure=="betweenness"))

plot(~value + poploading, subset(y2eff, factor=="f3" & measure=="strength"))
plot(~value + poploading, subset(y2eff, factor=="f1" & measure=="strength"))

cor.test(~value + poploading, subset(y2eff, factor=="f3" & measure=="strength"))
cor.test(~value + poploading, subset(y2eff, factor=="f1" & measure=="strength"))

cor.test(~value + poploading, subset(y2eff, factor=="f3" & measure=="closeness"))
cor.test(~value + poploading, subset(y2eff, factor=="f1" & measure=="closeness"))

plot(~value + poploading, subset(y2eff, factor=="f3" & measure=="closeness")) #insanely quadratic
plot(~value + poploading, subset(y2eff, factor=="f1" & measure=="closeness")) #insanely quadratic

#temporarily: the quadratic shapes here show that as an item cross-loads on two factors,
#it becomes highly central in the network

#try plotting a network at low, medium, and high levels of f3
qgraph(mlist_uniq[[1]]$adjmats$EBICglasso$average) #0.0 loading on f1, 0.80 loading on u (so y2 and y11 are part of u and not f1)
qgraph(mlist_uniq[[32]]$adjmats$EBICglasso$average) #0.56 loading on f1, 0.57 loading on u (about equal for y2 and y11)
qgraph(mlist_uniq[[65]]$adjmats$EBICglasso$average) #0.8 loading on f1, ~0 loading on u (so y2 and y11 are part of f1 and f2, respectively, and not u)

y2y11_edge <- sapply(mlist_uniq, function(cell) {
  cell$adjmats$EBICglasso$average["y2", "y11"]
})

loading_grid$uedge <- y2y11_edge
cor.test(~uedge + f_loading, loading_grid)
cor.test(~uedge + u_loading, loading_grid)

plot(~uedge + I(f_loading^2), loading_grid)
plot(~uedge + I(u_loading^2), loading_grid)

#look at how edges for other nodes are affected by the unique y2-y11 edge across loadings
#note that y1 y3 y4 etc. have an equal proportion of variation explained in each case


allf <- mlist_uniq[[65]]$adjmats$EBICglasso$average
allu <- mlist_uniq[[1]]$adjmats$EBICglasso$average
blend <- mlist_uniq[[32]]$adjmats$EBICglasso$average

allf_rmuitems <- allf[!rownames(allf) %in% c("y2", "y11"), !colnames(allf) %in% c("y2", "y11")]
f1_allf <- allf_rmuitems[paste0("y", c(1, 3:10)), paste0("y", c(1, 3:10))]
f1_allf <- f1_allf[lower.tri(f1_allf)]

allu_rmuitems <- allu[!rownames(allu) %in% c("y2", "y11"), !colnames(allu) %in% c("y2", "y11")]
f1_allu <- allu_rmuitems[paste0("y", c(1, 3:10)), paste0("y", c(1, 3:10))]
f1_allu <- f1_allu[lower.tri(f1_allu)]

blend_rmuitems <- blend[!rownames(blend) %in% c("y2", "y11"), !colnames(blend) %in% c("y2", "y11")]
f1_blend <- blend_rmuitems[paste0("y", c(1, 3:10)), paste0("y", c(1, 3:10))]
f1_blend <- f1_blend[lower.tri(f1_blend)]

t.test(f1_blend, f1_allf)
t.test(f1_blend, f1_allu)
t.test(f1_allf, f1_allu)

df <- data.frame(edge=c(f1_blend, f1_allf, f1_allu), 
                 condition=rep(c("blend", "allshared", "allunique"), each=length(f1_allf)), subj=1:length(f1_allf))

ggplot(df, aes(x=condition, y=edge)) + geom_boxplot()
#pearson correlation for comparison


library(ez)
ezANOVA(df, dv=edge, wid=.(subj), within=.(condition))
friedman.test(edge ~ condition | subj, df)

allf <- mlist_uniq[[65]]$adjmats$pearson$average
allu <- mlist_uniq[[1]]$adjmats$pearson$average
blend <- mlist_uniq[[32]]$adjmats$pearson$average

allf_rmuitems <- allf[!rownames(allf) %in% c("y2", "y11"), !colnames(allf) %in% c("y2", "y11")]
f1_allf <- allf_rmuitems[paste0("y", c(1, 3:10)), paste0("y", c(1, 3:10))]
f1_allf <- f1_allf[lower.tri(f1_allf)]

allu_rmuitems <- allu[!rownames(allu) %in% c("y2", "y11"), !colnames(allu) %in% c("y2", "y11")]
f1_allu <- allu_rmuitems[paste0("y", c(1, 3:10)), paste0("y", c(1, 3:10))]
f1_allu <- f1_allu[lower.tri(f1_allu)]

blend_rmuitems <- blend[!rownames(blend) %in% c("y2", "y11"), !colnames(blend) %in% c("y2", "y11")]
f1_blend <- blend_rmuitems[paste0("y", c(1, 3:10)), paste0("y", c(1, 3:10))]
f1_blend <- f1_blend[lower.tri(f1_blend)]

t.test(f1_blend, f1_allf)
t.test(f1_blend, f1_allu)
t.test(f1_allf, f1_allu)


```